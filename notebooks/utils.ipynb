{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import textdistance as td\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printUniqueTokens(series):\n",
    "    unique_series = series.unique()\n",
    "    token_count = {}\n",
    "    for a in unique_series:\n",
    "        tokens = a.split(' ')\n",
    "        for t in tokens:\n",
    "            if t not in token_count:\n",
    "                token_count[t] = 1\n",
    "            else:\n",
    "                token_count[t] += 1\n",
    "\n",
    "    for key, value in sorted(token_count.items(), key=lambda item: item[1]):\n",
    "        print(\"%s: %s\" % (key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f21c77eabe3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgolddupes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/restaurants_DPL.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdupedict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdupeRow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgolddupes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdupeRow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdupedict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdupedict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdupeRow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "golddupes = pd.read_csv('../data/work/restaurants_DPL.tsv', delimiter='\\t')\n",
    "dupedict = {}\n",
    "for i, dupeRow in golddupes.iterrows():\n",
    "    if dupeRow[0] not in dupedict:\n",
    "        dupedict[dupeRow[0]] = set()\n",
    "        dupedict[dupeRow[0]].add(dupeRow[0])\n",
    "    #dupedict[dupeRow[0]].add(dupeRow[0])\n",
    "    dupedict[dupeRow[0]].add(dupeRow[1])\n",
    "    \n",
    "dupesets = list(dupedict.values())\n",
    "dupeids = [y for x in dupesets for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareToGold(dfparam):\n",
    "    recdupes = list(dfparam[dfparam.id.map(len) > 1].id)\n",
    "    true_positive = []\n",
    "    false_negative = []\n",
    "    false_positive = []\n",
    "    for dupeset in dupesets:\n",
    "        if dupeset in recdupes:\n",
    "            true_positive.append(dupeset)\n",
    "        else:\n",
    "            false_negative.append(dupeset)\n",
    "    for recset in recdupes:\n",
    "        if recset not in dupesets:\n",
    "            false_positive.append(recset)\n",
    "    print(\"True positives: \" + str(len(true_positive)))\n",
    "    print(\"False negatives: \" + str(len(false_negative)))\n",
    "    print(\"False positives: \" + str(len(false_positive)))\n",
    "    precision = len(true_positive) / (len(true_positive) + len(false_positive))\n",
    "    recall = len(true_positive) / (len(true_positive) + len(false_negative))\n",
    "    fscore = (2*precision*recall) / (precision+recall)\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"Fscore: \" + str(fscore))\n",
    "    return true_positive, false_negative, false_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauce: https://www.geeksforgeeks.org/python-merge-list-with-common-elements-in-a-list-of-lists/\n",
    "#Importing  \n",
    "from collections import defaultdict \n",
    "  \n",
    "#merge function to  merge all sublist having common elements. \n",
    "def merge_common(lists): \n",
    "    neigh = defaultdict(set) \n",
    "    visited = set() \n",
    "    for each in lists: \n",
    "        for item in each: \n",
    "            neigh[item].update(each) \n",
    "    def comp(node, neigh = neigh, visited = visited, vis = visited.add): \n",
    "        nodes = set([node]) \n",
    "        next_node = nodes.pop \n",
    "        while nodes: \n",
    "            node = next_node() \n",
    "            vis(node) \n",
    "            nodes |= neigh[node] - visited \n",
    "            yield node \n",
    "    for node in neigh: \n",
    "        if node not in visited: \n",
    "            yield sorted(comp(node)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanGood(df):\n",
    "    data = df.copy()\n",
    "    data.type = data.type.fillna(\"\")\n",
    "    cityDict = {\n",
    "        \"w. hollywood\": \"west hollywood\",\n",
    "        \"new york city\": \"new york\",\n",
    "        \"west la\": \"los angeles\",\n",
    "        \"la\": \"los angeles\"\n",
    "    }\n",
    "\n",
    "    addressDict = {\n",
    "        r\"(ave|av)\": \"ave\",\n",
    "        r\"(blvd|blv)\": \"blvd\",\n",
    "        r\"(sts)\": \"st\",\n",
    "        r\"s\\.\": \"s\",\n",
    "        r\" ?between.*$\": \"\"\n",
    "    }\n",
    "\n",
    "    BRACKETS_REGEX = r\" ?\\(.*\\)\"\n",
    "    NON_ALPHA_OR_SPACE_REGEX = r\"[^a-zA-Z0-9 ]\"\n",
    "    NON_ALPHA_REGEX = r\"[^a-zA-Z0-9]\"\n",
    "    def preProcess(origData):\n",
    "        df = origData.copy()\n",
    "        #df = df.drop(labels=['id'], axis=1)\n",
    "        #df.set_index(\"id\", inplace = True)\n",
    "\n",
    "        for k, v in addressDict.items():\n",
    "            df.address = df.address.str.replace(k, v, case=False, regex=True)\n",
    "        for k, v in cityDict.items():\n",
    "            df.city = df.city.str.replace(k, v, case=False)\n",
    "        df.type = df.type.fillna(\"\")\n",
    "        df.type = df.type.str.replace(BRACKETS_REGEX, '', case=False)\\\n",
    "            .str.replace(r\"^.*[0-9] ?\", \"\", case=False)\n",
    "        df.phone = df.phone.str.replace(NON_ALPHA_REGEX, '', case=False)\n",
    "        df.name = df.name.str.replace(BRACKETS_REGEX, '', case=False)\n",
    "\n",
    "        df[\"cname\"] = df.name.copy()\n",
    "        df[\"caddress\"] = df.address.copy()\n",
    "        df.cname = df.cname.str.replace(NON_ALPHA_OR_SPACE_REGEX, '', case=False)\\\n",
    "            .str.replace('  the$', '', case=False)\n",
    "        df.caddress = df.caddress.str.replace(NON_ALPHA_OR_SPACE_REGEX, '', case=False)\n",
    "\n",
    "        #df = df.drop_duplicates([\"city\", \"phone\", \"type\", \"name\", \"address\"])\n",
    "        return df\n",
    "    preProcessedData = preProcess(data)    \n",
    "    groupedData = preProcessedData.groupby([\"city\", \"cname\", \"caddress\"]).agg(set).reset_index()\n",
    "    \n",
    "    from itertools import combinations\n",
    "\n",
    "    def bias(p1, p2):\n",
    "        if p1 in p2 or p2 in p1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def calcDistances(nameset):\n",
    "        alldists = []\n",
    "\n",
    "        combis = set(map(frozenset, combinations(set(nameset), 2)))\n",
    "        #for c in combis:\n",
    "        dists = []\n",
    "        dists = [(p1, p2, p1 == p2 and -1 or td.jaccard(p1, p2) + bias(p1, p2)) for p1, p2 in combis]\n",
    "        alldists += dists\n",
    "\n",
    "        #for someset in nameseries:\n",
    "        #    if len(someset) <= 1:\n",
    "        #        continue\n",
    "        #    dists = []\n",
    "        #    dists = [(p1, p2, p1 == p2 and -1 or td.jaccard(p1, p2) + bias(p1, p2)) for p1 in someset for p2 in someset]\n",
    "        #    alldists += dists\n",
    "        alldists.sort(key=lambda x: x[2], reverse=True)\n",
    "        return alldists\n",
    "\n",
    "    alldists = calcDistances(preProcessedData.cname.unique())\n",
    "    \n",
    "    def transformToEqualityRings(l):\n",
    "        sets = map(lambda x: set(x[0:2]), l)\n",
    "        return list(merge_common(sets)) \n",
    "\n",
    "    filtered = list(filter(lambda x: x[2] >= 0.70, alldists))\n",
    "    filtered\n",
    "    eqRing = transformToEqualityRings(filtered)\n",
    "    #uniquemappings = dict(map(lambda x: set(x[0:2]), filtered))\n",
    "    \n",
    "    def findClosestMatch(x):\n",
    "        for p1, p2, v in filtered:\n",
    "            if x == p1:\n",
    "                return p1\n",
    "            elif x == p2:\n",
    "                return p1\n",
    "        return x\n",
    "\n",
    "    def findClosestEqMatch(x):\n",
    "        for ring in eqRing:\n",
    "            for e in ring:\n",
    "                if e == x:\n",
    "                    return ring[0]\n",
    "        return x\n",
    "\n",
    "    def mapTextDistanceToUnique(df):\n",
    "        df[\"tdkey\"] = df.cname.copy()\n",
    "        #df.tdkey = df.tdkey.apply(findClosestMatch)\n",
    "        df.tdkey = df.tdkey.apply(findClosestEqMatch)\n",
    "        return df\n",
    "\n",
    "    tdData = preProcessedData.copy()\n",
    "    tdData = mapTextDistanceToUnique(tdData)\n",
    "    tdData\n",
    "    \n",
    "    dedupeData2 = tdData.groupby([\"phone\", \"tdkey\"]).agg(set).reset_index()\n",
    "    print(\"# SIMPLE GROUP\")\n",
    "    compareToGold(groupedData)\n",
    "    print(\"# GROUP BY PHONE\")\n",
    "    a, b, c = compareToGold(dedupeData2)\n",
    "    missing = [y for x in b for y in x]\n",
    "    print(tdData[tdData.id.isin(missing)])\n",
    "    return dedupeData2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}